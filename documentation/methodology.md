# Methodology

We will complete these tests with the following face detection model: MTCNN. That is the same our related work article uses.

## Related Work

One of the related work articles have split the data set into 50/50 for training and testing. The seccond article of the related work uses 5,000 images out of 13,000 images. However it is still unkown if they trained on 13,000 images and tested on 5,000 images.

## Plan

### 1. Prepaire and normalize the datasets

Prepaire and normalize the datasets to a unified structure. For our experiment to work we need to filter out persons with less the two images. This is so we can split tha dataset into known and unknown persons. The known persons will be the ones that have pictures in the DB, which we expect to get an identity of. The unknown persons will be the ones that do not have pictures in the DB, which we expect the identity of the person to be unknown. We will talk about the % of the split. However we will use a 80% of the known persons image in the database. With the following 20% as test data. With the unkown persons we will use all the images in the dataset as test data.

For example:
- We have 10 images of Brad Pitt and he is in the known group. 8 of the 10 images will be in the database and 2 of them will be in the test set.
- We have 10 images of Will Smith and he is in the unknown group. All 10 images will be in the test set.

DeepFace want to have the following structure on the data in their databse:
- ğŸ“ database
  - ğŸ“ person-name
    - ğŸ“· 001.jpg
    - ğŸ“· 002.jpg
    - ğŸ“· 003.jpg
    - ğŸ“· ...
  - ğŸ“ person-name
    - ğŸ“· 001.jpg
    - ğŸ“· 002.jpg
    - ğŸ“· 003.jpg
    - ğŸ“· ...

*We need to place the images in the correct folder structure for the DeepFace framework to work. Known and unkown persons in the correct folder. The above is an example of how the known persons are placed in the database.*

### 2. Run the experiment on all models.

We wil run the experiment on all models that are supported by the DeepFace framework. After each prediction we will save the result and store it until all tests are done for that model, Then for each model we will evaluate the results.

### 3. Evaluate the results.

We will use a confusion matrix to evaluate the results. This will help us to get a better and compareable result on the models.

| | Predicted Known | Predicted Unknown |
| --- | --- | --- |
| Actual Known | TP | FN |
| Actual Unknown | FP | TN |

For each test run we will compare different key figures and metrics. We will use the following metrics:
- Sensitivity / Recall
- Specificity
- Accuracy
- Precision
- F1-score

To be able to calculate the scores correct we need to save every result in an array which we later can send to the confusion matrix. The confusion matrix is a needed to calculate the scores correct.

*We can use scikit-learn to get the training and testing set. We can also use the confusion matrix from scikit-learn.*

TP = True Positive
TN = True Negative
FP = False Positive
FN = False Negative

[Read more about the Confusion Matrix](https://www.geeksforgeeks.org/machine-learning/confusion-matrix-machine-learning/)

### Sensitivity / Recall

Hur ofta systemet korrekt nekar igenkÃ¤nning nÃ¤r ansiktet inte tillhÃ¶r nÃ¥gon i databasen.

```markdown
Sensitivity=TP/(TP+FN)
```

### Specificity

Hur ofta systemet korrekt nekar igenkÃ¤nning nÃ¤r ansiktet inte tillhÃ¶r nÃ¥gon i databasen.

```markdown
Specificity=TN/(TN+FP)
```

### Accuracy

Om det finns mÃ¥nga fler negativa Ã¤n positiva fall kan accuracy vara hÃ¶g Ã¤ven om modellen Ã¤r dÃ¥lig pÃ¥ det viktiga.

```markdown
Accuracy=(TP+TN)/(TP+TN+FP+FN)
```

### Precision

Hur ofta systemet har rÃ¤tt nÃ¤r det sÃ¤ger att tvÃ¥ ansikten matchar.

```markdown
Precision=TP/(TP+FP)
```

### F1-score

Den visar hur vÃ¤l modellen presterar Ã¶verlag, utan att gynna bara precision eller bara sensitivity.

```markdown
F1=2âˆ—(Precisionâˆ—Sensitivity)/(Precision+Sensitivity)
```

## Things left to decide
- How to split the dataset into known and unknown persons.